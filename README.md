# LMUCS: A Lightweight LLM-Driven UAV Control System with Multimodal Perception

This repository contains the datasets, evaluation scripts, and experimental data for the paper: **"LMUCS: A Lightweight LLM-Driven UAV Control System with Multimodal Perception for an Autonomous Material Search and Localization"**.

LMUCS is a lightweight, multimodal perception and control system for unmanned aerial vehicles (UAVs). It is designed to address the significant challenge of integrating large language models (LLMs) and advanced visual perception on resource-constrained drone platforms to achieve fluid natural language interaction and real-time environmental awareness.

The system's core is a compact LLM, efficiently fine-tuned using Prompt Engineering and Low-Rank Adaptation (LoRA) techniques to accurately parse natural language commands. We also employ quantization to make running the LLM on airborne devices possible. For visual perception, LMUCS integrates a fine-tuned YOLOv11 model for object detection and the MiDaS monocular depth estimation algorithm to acquire spatial distance information.

This repository primarily contains the data and evaluation scripts related to the LLM fine-tuning and performance benchmarks presented in the paper.

## Key Contributions
1.  **Lightweighting Methodology:** We validated an efficiency-first model lightweighting approach. By combining LoRA  and Q4 quantization , our fine-tuned compact LLM achieves high instruction parsing accuracy (over 90%) while reducing inference latency, demonstrating the potential for real-time command execution on edge devices.
2.  **Lightweight Perception:** We designed a lightweight perception solution integrating a fine-tuned YOLOv11 for robust object detection and MiDaS for monocular depth estimation.
3.  **Novel Dataset Generation:** We designed and executed an innovative two-stage prompt engineering data generation pipeline to build and release the first scalable, open-source dataset for drone control, covering flight, programmatic, and search tasks. This dataset, containing tens of thousands of high-quality "natural language-control command" pairs, alleviates the scarcity of domain-specific data.

## Repository Structure

This repository contains the data and scripts used to produce the evaluation results in our paper.

* **`./llm_finetune_comparison_summary.xlsx`**: An Excel spreadsheet summarizing the key performance metrics (accuracy, latency) for all tested LLMs, both before and after fine-tuning. This file contains the source data for tables in the paper.


* **`./finetuning_logs/`**: Contains the raw training logs (e.g., loss curves, steps) generated by MS-Swift during the LoRA fine-tuning process for each model.

* **`./dataset_generation/`**: This is the core dataset contribution.
    * `train_dataset.jsonl` / `val_dataset.jsonl`: The final, high-quality "instruction-action" pair datasets used for fine-tuning (~38,000 samples).
    * `./generation_pipeline/`: Contains the intermediate data files and scripts used in our two-stage prompt engineering pipeline to generate the final dataset.

* **`./evaluation_llm_accuracy/`**: Contains scripts and raw model outputs for benchmarking LLM instruction parsing accuracy.
    * `analyze_accuracy_and_time.py`: Python script to parse the output files and calculate "Exact Match" (EM) and "Contains Answer" accuracy, as shown in Figure 4c and Table 1.
    * `./before/`: Raw `.jsonl` outputs from the *un-tuned* (base) models.
    * `./after/`: Raw `.jsonl` outputs from our *fine-tuned* models.

* **`./evaluation_latency_quantization/`**: Scripts and logs for benchmarking model latency and the performance of quantized models on edge devices.
    * `test_edge_latency_quantization.py`: Script used to run benchmarks on the Jetson Xavier NX.
    * `test_cloud_api.py`: Script used to benchmark the cloud API (DeepSeek v3).
    * `parse_..._logs.py`: Scripts to parse the raw log files and calculate average latency and IPS (Inferences Per Second).
    * `*.jsonl`: Log files and test data used for these benchmarks, which produced the results in Table 2  and Table 3.

* **`. /model/`**: stores large language models and YOLO models
    * `Qwen2.5_0.5b-droneq4/qwen2_5-0.5B-after-Q4_0.gguf`: qwen2.5_0.5b is a large language model that has been fine-tuned with data and can be deployed using ollama
    * `yolo/*.pt`: YOLO model trained on dataset
    * `yolo/*.yaml`: The structure of the yolo model
    * `yolo/dataset.zip`: dataset for yolo model


## Usage

Please follow these steps to install and run this model.

### 1\. Install Ollama

First, you need to install Ollama on your computer.

  * Visit the official Ollama download page: [https://ollama.com/download](https://ollama.com/download)
  * Download and install the appropriate version for your operating system (Windows, macOS, or Linux).

### 2\. Running the Model

Please follow these steps exactly:

#### Step 1: Start the Ollama Service

1.  Open a terminal or Command Prompt.
2.  Run the following command to start the Ollama service:
    ```bash
    ollama serve
    ```
3.  **Note:** After starting the service, **keep this terminal window open**. Do not close it.

#### Step 2: Create the Local Model (One-Time Setup)

1.  **Open a separate, new terminal window** (do not use the one running `ollama serve`).
2.  Use the `cd` command to navigate to the **root directory of this project** (where you downloaded it).
3.  Next, navigate into the specific model file directory:
    ```bash
    cd ./model/qwen2_5-0_5b_drone_q4
    ```
4.  Run the following command. Ollama will create (register) the model locally based on the `Modelfile_q4` file.
    ```bash
    ollama create qwen2.5_0.5b_drone_q4 -f Modelfile_q4
    ```
    *(You only need to do this step the very first time you run it. Ollama will remember the model afterward.)*

#### Step 3: Run the Model and Start Chatting

1.  In the **same terminal from Step 2** (after the model creation is successful), run the following command to start the model:
    ```bash
    ollama run qwen2.5_0.5b_drone_q4
    ```
2.  When the terminal shows a `>>>` prompt, the model is ready. You can now start sending it instructions.

### 3\. How to Ask (Prompt Examples)

This model requires specific prompt formats to understand your intent correctly. Please refer to the following examples to structure your queries.

- [prompt formats](./dataset_generation/generation_pipeline/train_prompt.json): `'./dataset_generation/generation_pipeline/train_prompt.json'`

-----

#### Example 1: Task Classification

Copy and paste the **entire text block below** into your terminal and press Enter:

```
Classify the input and respond with 'A.', 'B.', 'C.', or 'D.': \n- 'A.': Search tasks (e.g., \"Search around the pizza.\"). \n- 'B.': Flight command control instructions (e.g., \"Fly up 20 centimeters.\"). \n- 'C.': Program control instructions (e.g., \"Resume the previously paused flight control task.\"). \n- 'D.': Other types (e.g., \"What’s the weather today?\"). \nuser input: Seek out the skis with haste and precision.\nresponse:
```

#### Example 2: Drone Command Translation

Copy and paste the **entire text block below** into your terminal and press Enter:

```
Translate user input into drone commands: \n- Format: `[command] [value] [unit].` Commands include: take_off; land; move_forward/back/left/right/up/down x cm/m/in/ft; turn_left/right x degrees. \n- If no command, return `None.` If value missing, return `Missing [command].` \n- Separate commands with semicolons and end with a period.\nuser input: Hover and rotate right 45 degrees, then move it 200 centimeters forward, next push it forward, after that fly back 3 meters, and lower to ground level.\nresponse:
```


## How to Cite

If you find this work useful in your research, please consider citing our paper:

```bibtex
@article{LMUCS2025,
  title   = {LMUCS：用于自主物资搜索与定位的具有多模态感知的轻型LLM驱动无人机控制系统 (LMUCS: A Lightweight LLM-Driven UAV Control System with Multimodal Perception for Autonomous Material Search and Localization)},
  author  = {Peng Wang and Zhenhao Shuai and Kun Wang and Qi-Chao Li and Jian-Wei Shuai and Fang-Fu Ye},
  journal = {Aerospace Science and Technology},
  year    = {2025}
}